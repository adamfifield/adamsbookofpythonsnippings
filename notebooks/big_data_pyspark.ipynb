{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1cad7e7",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ”¥ Big Data Processing with PySpark\n",
    "\n",
    "This notebook provides **code templates and checklists** for **handling large-scale datasets using PySpark**.\n",
    "\n",
    "### ðŸ”¹ Whatâ€™s Covered:\n",
    "- Initializing a PySpark session\n",
    "- Loading and processing large datasets\n",
    "- Performing transformations & aggregations\n",
    "- Optimizing performance for distributed computing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure required libraries are installed (Uncomment if necessary)\n",
    "# !pip install pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2735670",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸš€ Initializing PySpark\n",
    "\n",
    "âœ… Set up a **Spark session** to enable distributed computing.  \n",
    "âœ… Configure **memory allocation & parallelism** for efficiency.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ef644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder     .appName(\"BigDataProcessing\")     .config(\"spark.executor.memory\", \"2g\")     .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69036e14",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“‚ Loading Large Datasets\n",
    "\n",
    "âœ… Load **CSV, Parquet, JSON** files efficiently.  \n",
    "âœ… Use **schema definition** to avoid automatic inference overhead.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define schema for dataset\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Load data (replace with actual file path)\n",
    "df = spark.read.csv(\"large_dataset.csv\", schema=schema, header=True)\n",
    "\n",
    "# Show a sample\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637933aa",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ”„ Data Transformations\n",
    "\n",
    "âœ… Perform **filtering, selection, and column transformations**.  \n",
    "âœ… Use **optimized Spark functions** over Pandas-like operations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0158cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter dataset (age > 30)\n",
    "df_filtered = df.filter(col(\"age\") > 30)\n",
    "\n",
    "# Select specific columns\n",
    "df_selected = df_filtered.select(\"id\", \"name\")\n",
    "\n",
    "df_selected.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3701113",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“Š Aggregations & Grouping\n",
    "\n",
    "âœ… Perform **group-by operations** on large datasets.  \n",
    "âœ… Use **Sparkâ€™s built-in aggregation functions** for efficiency.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eced0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregate data: Count people by age\n",
    "df_grouped = df.groupBy(\"age\").count()\n",
    "\n",
    "# Show results\n",
    "df_grouped.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d35037",
   "metadata": {},
   "source": [
    "\n",
    "## âš¡ Optimizing Performance\n",
    "\n",
    "âœ… **Cache data** when reusing DataFrames.  \n",
    "âœ… Use **Parquet** instead of CSV for better speed.  \n",
    "âœ… **Repartition data** to balance workload across nodes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fddebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cache DataFrame for repeated use\n",
    "df.cache()\n",
    "\n",
    "# Save as Parquet format for optimized storage\n",
    "df.write.parquet(\"large_dataset.parquet\")\n",
    "\n",
    "# Repartition dataset to optimize parallelism\n",
    "df_repartitioned = df.repartition(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e45931",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Best Practices & Common Pitfalls\n",
    "\n",
    "- **Avoid small partitions**: Too many partitions slow down performance.  \n",
    "- **Use built-in Spark functions**: Avoid UDFs unless necessary.  \n",
    "- **Monitor memory usage**: Ensure efficient execution with `.explain()` or Spark UI.  \n",
    "- **Prefer Parquet over CSV**: Faster read/write operations.  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
