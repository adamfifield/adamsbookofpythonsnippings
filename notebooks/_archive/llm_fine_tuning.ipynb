{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d29bab9",
   "metadata": {},
   "source": [
    "\n",
    "# üß† Large Language Model (LLM) Inference & Fine-Tuning\n",
    "\n",
    "This notebook provides **code templates and checklists** for **running and fine-tuning large language models (LLMs) like GPT and BERT**.\n",
    "\n",
    "### üîπ What‚Äôs Covered:\n",
    "- Running LLM inference with Hugging Face Transformers\n",
    "- Fine-tuning a transformer model on custom data\n",
    "- Using LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
    "- Deploying fine-tuned models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0dc8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure required libraries are installed (Uncomment if necessary)\n",
    "# !pip install transformers datasets torch accelerate peft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516e1ab",
   "metadata": {},
   "source": [
    "\n",
    "## üöÄ Running LLM Inference with Hugging Face\n",
    "\n",
    "‚úÖ Load a **pretrained LLM** for text generation or classification.  \n",
    "‚úÖ Use **pipeline** API for quick inference.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a GPT-2 text generation model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Once upon a time,\"\n",
    "output = generator(prompt, max_length=50)\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38442305",
   "metadata": {},
   "source": [
    "\n",
    "## üèãÔ∏è Fine-Tuning a Transformer Model\n",
    "\n",
    "‚úÖ Use **Hugging Face Trainer** for efficient fine-tuning.  \n",
    "‚úÖ Prepare a **custom dataset** for fine-tuning.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load a dataset (example: IMDB sentiment analysis dataset)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Load a pretrained model for text classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"].shuffle().select(range(1000)),  # Using a subset for speed\n",
    "    eval_dataset=dataset[\"test\"].shuffle().select(range(500))\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70cce63",
   "metadata": {},
   "source": [
    "\n",
    "## üî¨ Using LoRA (Low-Rank Adaptation) for Efficient Fine-Tuning\n",
    "\n",
    "‚úÖ Reduce **memory usage & training cost**.  \n",
    "‚úÖ Fine-tune only a subset of model weights.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define LoRA config\n",
    "lora_config = LoraConfig(r=8, lora_alpha=32, target_modules=[\"query\", \"value\"], lora_dropout=0.05)\n",
    "\n",
    "# Apply LoRA to the model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Train as usual using Trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"].shuffle().select(range(1000)),\n",
    "    eval_dataset=dataset[\"test\"].shuffle().select(range(500))\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb1222",
   "metadata": {},
   "source": [
    "\n",
    "## üåç Deploying Fine-Tuned Models\n",
    "\n",
    "‚úÖ Save the fine-tuned model for later use.  \n",
    "‚úÖ Upload to **Hugging Face Model Hub** or deploy via **FastAPI**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f82d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_model\")\n",
    "\n",
    "# Upload to Hugging Face Hub (Requires authentication)\n",
    "# !huggingface-cli login\n",
    "# model.push_to_hub(\"your-username/fine-tuned-model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca7553c",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Best Practices & Common Pitfalls\n",
    "\n",
    "- **Fine-tune efficiently**: Use **LoRA or quantization** to reduce memory usage.  \n",
    "- **Use a small dataset first**: Avoid expensive full-scale fine-tuning on first runs.  \n",
    "- **Monitor loss carefully**: Overfitting happens quickly on small datasets.  \n",
    "- **Use mixed precision training**: Speeds up training without sacrificing accuracy.  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
