{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938afb36",
   "metadata": {},
   "source": [
    "\n",
    "# ü§ñ Fine-Tuning Deep Learning Models\n",
    "\n",
    "This notebook provides **code templates and checklists** for **fine-tuning pretrained deep learning models** to improve performance.\n",
    "\n",
    "### üîπ What‚Äôs Covered:\n",
    "- Loading pretrained models\n",
    "- Freezing and unfreezing layers\n",
    "- Adjusting learning rates & regularization\n",
    "- Transfer learning with custom datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure required libraries are installed (Uncomment if necessary)\n",
    "# !pip install torch torchvision tensorflow keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae9140",
   "metadata": {},
   "source": [
    "\n",
    "## üì• Loading a Pretrained Model\n",
    "\n",
    "‚úÖ Use pretrained models to **accelerate training**.  \n",
    "‚úÖ Freeze early layers and fine-tune later ones.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c93c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Print model structure\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35dda09",
   "metadata": {},
   "source": [
    "\n",
    "## üèóÔ∏è Freezing & Unfreezing Layers\n",
    "\n",
    "‚úÖ Freeze layers to **retain learned features**.  \n",
    "‚úÖ Unfreeze only selected layers for fine-tuning.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Freeze all layers except the last few\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze last few layers\n",
    "for param in list(model.parameters())[-10:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Model ready for fine-tuning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6016de1",
   "metadata": {},
   "source": [
    "\n",
    "## ‚öôÔ∏è Adjusting Learning Rates\n",
    "\n",
    "‚úÖ Use **lower learning rates** for fine-tuning.  \n",
    "‚úÖ Apply **differential learning rates** for different layers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056fd7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define optimizer with lower learning rate for pretrained layers\n",
    "optimizer = optim.Adam([\n",
    "    {\"params\": model.fc.parameters(), \"lr\": 1e-3},  # Fine-tuning layer\n",
    "    {\"params\": model.layer4.parameters(), \"lr\": 1e-4},  # Intermediate layers\n",
    "], lr=1e-5)\n",
    "\n",
    "print(\"Optimizer configured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a934a4c",
   "metadata": {},
   "source": [
    "\n",
    "## üèãÔ∏è‚Äç‚ôÇÔ∏è Transfer Learning with Custom Dataset\n",
    "\n",
    "‚úÖ Replace the final layer for **custom classification tasks**.  \n",
    "‚úÖ Ensure dataset is properly preprocessed.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99700162",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Modify the final layer for 5-class classification\n",
    "num_classes = 5\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model ready for training on new dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303017a",
   "metadata": {},
   "source": [
    "\n",
    "## üõ°Ô∏è Regularization Techniques\n",
    "\n",
    "‚úÖ Use **dropout** to prevent overfitting.  \n",
    "‚úÖ Apply **weight decay (L2 regularization)** to penalize large weights.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca576634",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adding dropout layer\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "# Apply weight decay in optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ee144",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Best Practices & Common Pitfalls\n",
    "\n",
    "- **Don't unfreeze too many layers at once**: It may lead to catastrophic forgetting.  \n",
    "- **Use a small learning rate**: Large updates can erase pretrained knowledge.  \n",
    "- **Regularization matters**: Avoid overfitting with dropout and weight decay.  \n",
    "- **Test performance iteratively**: Save intermediate models and compare results.  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
