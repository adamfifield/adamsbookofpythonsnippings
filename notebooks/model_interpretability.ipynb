{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965d4566",
   "metadata": {},
   "source": [
    "\n",
    "# üîç Model Interpretability & Explainability\n",
    "\n",
    "This notebook provides **code templates and checklists** for **understanding how ML models make decisions** using explainability techniques.\n",
    "\n",
    "### üîπ What‚Äôs Covered:\n",
    "- Feature importance analysis\n",
    "- SHAP (SHapley Additive Explanations)\n",
    "- LIME (Local Interpretable Model-Agnostic Explanations)\n",
    "- Partial dependence plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure required libraries are installed (Uncomment if necessary)\n",
    "# !pip install pandas numpy sklearn shap lime matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174afe70",
   "metadata": {},
   "source": [
    "\n",
    "## üå≥ Feature Importance with Tree-Based Models\n",
    "\n",
    "‚úÖ Use **feature importance scores** from decision trees.  \n",
    "‚úÖ Identify which features contribute most to model predictions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset (Replace with actual data)\n",
    "df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Convert to DataFrame for easier visualization\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by=\"Importance\", ascending=False)\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f905f",
   "metadata": {},
   "source": [
    "\n",
    "## üõ†Ô∏è SHAP (SHapley Additive Explanations)\n",
    "\n",
    "‚úÖ Use **SHAP values** to explain individual predictions.  \n",
    "‚úÖ Identify how each feature **pushes predictions higher or lower**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85582a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Visualize SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597fe8d7",
   "metadata": {},
   "source": [
    "\n",
    "## üî¨ LIME (Local Interpretable Model-Agnostic Explanations)\n",
    "\n",
    "‚úÖ Use **LIME** to explain individual predictions.  \n",
    "‚úÖ Works well with black-box models like deep learning & ensembles.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Create LIME explainer\n",
    "explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns, class_names=['Class 0', 'Class 1'], mode=\"classification\")\n",
    "\n",
    "# Explain a single prediction\n",
    "idx = 0  # Index of sample to explain\n",
    "exp = explainer.explain_instance(X_test.iloc[idx].values, model.predict_proba)\n",
    "exp.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4039fdf3",
   "metadata": {},
   "source": [
    "\n",
    "## üìä Partial Dependence Plots (PDP)\n",
    "\n",
    "‚úÖ Show how a **single feature influences model predictions**.  \n",
    "‚úÖ Helps identify **non-linear relationships** between features & targets.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622eaba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate partial dependence plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "plot_partial_dependence(model, X_train, features=[0, 1], feature_names=X_train.columns, ax=ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc67ff",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Best Practices & Common Pitfalls\n",
    "\n",
    "- **Choose the right tool**: SHAP is more robust, but LIME is faster.  \n",
    "- **Consider computational cost**: SHAP can be slow for large datasets.  \n",
    "- **Compare results**: Feature importance, SHAP, and LIME may give different insights.  \n",
    "- **Check for bias**: If a feature is overly dominant, your model might be biased.  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
