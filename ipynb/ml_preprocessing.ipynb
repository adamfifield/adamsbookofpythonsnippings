{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95db59d2",
   "metadata": {},
   "source": [
    "\n",
    "# üéØ Preprocessing for Machine Learning Models\n",
    "\n",
    "This notebook provides **code templates and checklists** for **preparing datasets for machine learning**. Proper preprocessing ensures models receive clean, well-structured input data.\n",
    "\n",
    "### üîπ What‚Äôs Covered:\n",
    "- Handling missing data\n",
    "- Encoding categorical variables\n",
    "- Feature scaling & normalization\n",
    "- Splitting data for training & testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db124a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure required libraries are installed (Uncomment if necessary)\n",
    "# !pip install pandas numpy sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ef8c78",
   "metadata": {},
   "source": [
    "\n",
    "## üö´ Handling Missing Data\n",
    "\n",
    "‚úÖ Identify missing values in the dataset.  \n",
    "‚úÖ Decide whether to **drop** or **impute** missing values.  \n",
    "‚úÖ Choose the right imputation strategy (mean, median, mode).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63639a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample dataset with missing values\n",
    "df = pd.DataFrame({\n",
    "    'age': [25, 30, np.nan, 40, 35],\n",
    "    'salary': [50000, 70000, 90000, np.nan, 65000],\n",
    "    'city': ['NY', 'LA', 'SF', 'NY', np.nan]\n",
    "})\n",
    "\n",
    "# Identify missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Impute missing values for numerical columns (mean strategy)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[['age', 'salary']] = imputer.fit_transform(df[['age', 'salary']])\n",
    "\n",
    "# Impute missing values for categorical columns (most frequent value)\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "df[['city']] = imputer_cat.fit_transform(df[['city']])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d171fb",
   "metadata": {},
   "source": [
    "\n",
    "## üî§ Encoding Categorical Variables\n",
    "\n",
    "‚úÖ Convert categorical features into numerical representations.  \n",
    "‚úÖ Use **One-Hot Encoding** for non-ordinal categories.  \n",
    "‚úÖ Use **Label Encoding** for ordinal categories.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aeb4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df, columns=['city'], drop_first=True)\n",
    "\n",
    "# Label Encoding (Example for ordinal categories)\n",
    "le = LabelEncoder()\n",
    "df['encoded_city'] = le.fit_transform(df['city'])\n",
    "\n",
    "print(df_encoded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be51cb",
   "metadata": {},
   "source": [
    "\n",
    "## üìè Feature Scaling & Normalization\n",
    "\n",
    "‚úÖ Normalize numerical features to ensure comparability.  \n",
    "‚úÖ Use **Min-Max Scaling** (0-1 range) or **Standardization** (Z-score).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b261909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "df[['salary_scaled']] = scaler.fit_transform(df[['salary']])\n",
    "\n",
    "# Standardization (Z-score normalization)\n",
    "scaler = StandardScaler()\n",
    "df[['salary_standardized']] = scaler.fit_transform(df[['salary']])\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff95be4",
   "metadata": {},
   "source": [
    "\n",
    "## üìÇ Splitting Data for Training & Testing\n",
    "\n",
    "‚úÖ Ensure a **proper split** between training & testing data.  \n",
    "‚úÖ Use **stratified sampling** for imbalanced classification problems.  \n",
    "‚úÖ Avoid **data leakage** when preparing features.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df.drop(columns=['salary'])\n",
    "y = df['salary']\n",
    "\n",
    "# Split dataset into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f30a1",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Best Practices & Common Pitfalls\n",
    "\n",
    "- **Ensure consistent scaling**: Apply the same scaler used on training data to test data.  \n",
    "- **Check for class imbalance**: Consider stratified splits for imbalanced datasets.  \n",
    "- **Avoid data leakage**: Don't use test data when normalizing or encoding training data.  \n",
    "- **Use pipelines**: Combine preprocessing steps using `sklearn.pipeline.Pipeline` for cleaner code.  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
